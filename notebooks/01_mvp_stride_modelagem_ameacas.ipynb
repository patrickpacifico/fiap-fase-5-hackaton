{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MVP: Modelagem de Amea\u00e7as com IA (STRIDE)\n",
        "\n",
        "Este notebook cria um MVP para detectar componentes em diagramas de arquitetura e gerar amea\u00e7as e contramedidas baseadas em STRIDE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get -qq update && apt-get -qq install -y tesseract-ocr\n",
        "!pip -q install ultralytics opencv-python pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import pytesseract\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Clona/atualiza o reposit\u00f3rio e garante download via Git LFS\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "\n",
        "DIR_REPO = '/content/fiap-fase-5-hackaton'\n",
        "if not os.path.isdir(DIR_REPO):\n",
        "    !git clone https://github.com/patrickpacifico/fiap-fase-5-hackaton.git\n",
        "else:\n",
        "    !git -C /content/fiap-fase-5-hackaton pull\n",
        "\n",
        "# Garante git-lfs instalado\n",
        "try:\n",
        "    subprocess.run(['git', 'lfs', 'install'], check=True)\n",
        "except Exception:\n",
        "    !apt-get -qq update && apt-get -qq install -y git-lfs\n",
        "    !git lfs install\n",
        "\n",
        "# Baixa arquivos LFS\n",
        "!git -C /content/fiap-fase-5-hackaton lfs pull\n",
        "\n",
        "print('Arquivos no repo:')\n",
        "!ls /content/fiap-fase-5-hackaton\n",
        "\n",
        "# Procura o zip no repo\n",
        "encontrados = glob.glob('/content/fiap-fase-5-hackaton/**/arqsof_dataset.zip', recursive=True)\n",
        "print('Zips encontrados:', encontrados)\n",
        "\n",
        "CAMINHO_ZIP = encontrados[0] if encontrados else ''\n",
        "print('CAMINHO_ZIP definido como:', CAMINHO_ZIP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Escolha do modo\n",
        "Use `treino` quando tiver dataset anotado (YOLO/COCO).\n",
        "Use `inferencia` quando tiver apenas uma imagem do diagrama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODO = 'inferencia'  # 'treino' ou 'inferencia'\n",
        "print('Modo selecionado:', MODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3A) Upload do dataset (somente treino)\n",
        "Envie um .zip contendo o dataset. Estruturas aceitas:\n",
        "- YOLO padr\u00e3o: images/train, images/val, labels/train, labels/val\n",
        "- YOLO sem splits: images/ e labels/ (o notebook cria train/val)\n",
        "- VOC: imagens + arquivos .xml (o notebook converte para YOLO)\n",
        "- Pasta \u00fanica com imagens + .txt (o notebook organiza em YOLO)\n",
        "\n",
        "Exemplo comum: `src/dataset/dataset_augmented` com arquivos `.png/.jpg` e `.xml`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Upload do dataset ou uso do zip localizado no repo\n",
        "if MODO != 'treino':\n",
        "    print('Pulando upload de dataset (modo n\u00e3o \u00e9 treino).')\n",
        "else:\n",
        "    if CAMINHO_ZIP:\n",
        "        NOME_ZIP = CAMINHO_ZIP\n",
        "        print('Usando zip local:', NOME_ZIP)\n",
        "    else:\n",
        "        ARQUIVOS = files.upload()\n",
        "        NOME_ZIP = next(iter(ARQUIVOS))\n",
        "\n",
        "    DIR_EXTRACAO = Path('/content/dataset_zip')\n",
        "    DIR_EXTRACAO.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(NOME_ZIP, 'r') as zf:\n",
        "        zf.extractall(DIR_EXTRACAO)\n",
        "\n",
        "    print('Dataset extra\u00eddo em:', DIR_EXTRACAO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Prepara\u00e7\u00e3o autom\u00e1tica do dataset para YOLO (suporta VOC .xml)\n",
        "import random\n",
        "import shutil\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "\n",
        "EXTENSOES_IMAGEM = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
        "\n",
        "def possui_estrutura_yolo(dir_base):\n",
        "    return all((dir_base / p).exists() for p in [\n",
        "        'images/train', 'images/val', 'labels/train', 'labels/val'\n",
        "    ])\n",
        "\n",
        "def possui_images_labels_sem_split(dir_base):\n",
        "    return (dir_base / 'images').exists() and (dir_base / 'labels').exists()\n",
        "\n",
        "def listar_imagens(pasta):\n",
        "    return [p for p in pasta.rglob('*') if p.suffix.lower() in EXTENSOES_IMAGEM]\n",
        "\n",
        "def encontrar_pasta_com_pares(dir_base):\n",
        "    melhores = (0, None)\n",
        "    for pasta in [dir_base] + [p for p in dir_base.rglob('*') if p.is_dir()]:\n",
        "        imgs = [p for p in pasta.iterdir() if p.is_file() and p.suffix.lower() in EXTENSOES_IMAGEM]\n",
        "        if not imgs:\n",
        "            continue\n",
        "        pares = [img for img in imgs if (pasta / f'{img.stem}.txt').exists()]\n",
        "        if len(pares) > melhores[0]:\n",
        "            melhores = (len(pares), pasta)\n",
        "    return melhores[1]\n",
        "\n",
        "def coletar_xmls_recursivo(dir_base):\n",
        "    return [p for p in Path(dir_base).rglob('*') if p.is_file() and p.suffix.lower() == '.xml']\n",
        "\n",
        "def mapear_imagens_recursivo(dir_base):\n",
        "    imagens = listar_imagens(dir_base)\n",
        "    por_stem = {}\n",
        "    por_nome = {}\n",
        "    for img in imagens:\n",
        "        por_stem.setdefault(img.stem, img)\n",
        "        por_nome.setdefault(img.name, img)\n",
        "    return imagens, por_stem, por_nome\n",
        "\n",
        "def criar_estrutura_yolo(pares, dir_destino, proporcao_treino=0.8, seed=42):\n",
        "    random.seed(seed)\n",
        "    random.shuffle(pares)\n",
        "    corte = int(len(pares) * proporcao_treino)\n",
        "    treino, validacao = pares[:corte], pares[corte:]\n",
        "\n",
        "    (dir_destino / 'images/train').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'images/val').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'labels/train').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'labels/val').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def copiar(pares, split):\n",
        "        for img, label in pares:\n",
        "            shutil.copy2(img, dir_destino / f'images/{split}/{img.name}')\n",
        "            shutil.copy2(label, dir_destino / f'labels/{split}/{label.name}')\n",
        "\n",
        "    copiar(treino, 'train')\n",
        "    copiar(validacao, 'val')\n",
        "\n",
        "def ler_voc(xml_path):\n",
        "    root = ET.parse(xml_path).getroot()\n",
        "    size = root.find('size')\n",
        "    w = int(size.find('width').text)\n",
        "    h = int(size.find('height').text)\n",
        "    objetos = []\n",
        "    for obj in root.findall('object'):\n",
        "        nome = obj.find('name').text.strip()\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = float(bbox.find('xmin').text)\n",
        "        ymin = float(bbox.find('ymin').text)\n",
        "        xmax = float(bbox.find('xmax').text)\n",
        "        ymax = float(bbox.find('ymax').text)\n",
        "        x = ((xmin + xmax) / 2) / w\n",
        "        y = ((ymin + ymax) / 2) / h\n",
        "        bw = (xmax - xmin) / w\n",
        "        bh = (ymax - ymin) / h\n",
        "        objetos.append((nome, x, y, bw, bh))\n",
        "    return objetos\n",
        "\n",
        "def obter_nome_arquivo_voc(xml_path):\n",
        "    try:\n",
        "        root = ET.parse(xml_path).getroot()\n",
        "        filename = root.find('filename')\n",
        "        if filename is not None and filename.text:\n",
        "            return Path(filename.text.strip()).name\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def coletar_classes_voc(xmls):\n",
        "    classes = []\n",
        "    vistos = set()\n",
        "    for xml in xmls:\n",
        "        for nome, *_ in ler_voc(xml):\n",
        "            if nome not in vistos:\n",
        "                vistos.add(nome)\n",
        "                classes.append(nome)\n",
        "    return classes\n",
        "\n",
        "def encontrar_imagem_para_xml(xml_path, por_stem, por_nome):\n",
        "    stem = xml_path.stem\n",
        "    if stem in por_stem:\n",
        "        return por_stem[stem]\n",
        "    nome_arquivo = obter_nome_arquivo_voc(xml_path)\n",
        "    if nome_arquivo and nome_arquivo in por_nome:\n",
        "        return por_nome[nome_arquivo]\n",
        "    return None\n",
        "\n",
        "def converter_voc_para_yolo(xmls, dir_destino, por_stem, por_nome, proporcao_treino=0.8, seed=42, nomes_classes=None):\n",
        "    pares = []\n",
        "    for xml in xmls:\n",
        "        img = encontrar_imagem_para_xml(xml, por_stem, por_nome)\n",
        "        if img is not None:\n",
        "            pares.append((img, xml))\n",
        "\n",
        "    if not pares:\n",
        "        raise FileNotFoundError('Nenhuma imagem correspondente aos XMLs foi encontrada.')\n",
        "\n",
        "    if not nomes_classes:\n",
        "        nomes_classes = coletar_classes_voc([xml for _, xml in pares])\n",
        "\n",
        "    mapa = {nome: i for i, nome in enumerate(nomes_classes)}\n",
        "\n",
        "    random.seed(seed)\n",
        "    random.shuffle(pares)\n",
        "    corte = int(len(pares) * proporcao_treino)\n",
        "    treino, validacao = pares[:corte], pares[corte:]\n",
        "\n",
        "    (dir_destino / 'images/train').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'images/val').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'labels/train').mkdir(parents=True, exist_ok=True)\n",
        "    (dir_destino / 'labels/val').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def processar(pares, split):\n",
        "        for img, xml in pares:\n",
        "            shutil.copy2(img, dir_destino / f'images/{split}/{img.name}')\n",
        "            objetos = ler_voc(xml)\n",
        "            linhas = []\n",
        "            for nome, x, y, bw, bh in objetos:\n",
        "                if nome not in mapa:\n",
        "                    continue\n",
        "                linhas.append(f'{mapa[nome]} {x:.6f} {y:.6f} {bw:.6f} {bh:.6f}')\n",
        "            label_path = dir_destino / f'labels/{split}/{img.stem}.txt'\n",
        "            label_path.write_text('\\n'.join(linhas), encoding='utf-8')\n",
        "\n",
        "    processar(treino, 'train')\n",
        "    processar(validacao, 'val')\n",
        "\n",
        "    return dir_destino, nomes_classes\n",
        "\n",
        "def preparar_dataset_yolo(dir_base):\n",
        "    if possui_estrutura_yolo(dir_base):\n",
        "        return dir_base, None\n",
        "\n",
        "    if possui_images_labels_sem_split(dir_base):\n",
        "        imgs = listar_imagens(dir_base / 'images')\n",
        "        pares = []\n",
        "        for img in imgs:\n",
        "            label = (dir_base / 'labels' / f'{img.stem}.txt')\n",
        "            if label.exists():\n",
        "                pares.append((img, label))\n",
        "        if not pares:\n",
        "            raise FileNotFoundError('Nenhum par imagem/label encontrado em images/ e labels/.')\n",
        "\n",
        "        dir_yolo = Path('/content/data_yolo')\n",
        "        criar_estrutura_yolo(pares, dir_yolo)\n",
        "        return dir_yolo, None\n",
        "\n",
        "    xmls = coletar_xmls_recursivo(dir_base)\n",
        "    if xmls:\n",
        "        imagens, por_stem, por_nome = mapear_imagens_recursivo(dir_base)\n",
        "        print(f'Total de XMLs encontrados: {len(xmls)}')\n",
        "        print(f'Total de imagens encontradas: {len(imagens)}')\n",
        "        dir_yolo = Path('/content/data_yolo')\n",
        "        nomes_classes = None\n",
        "        if 'NOMES_CLASSES' in globals() and NOMES_CLASSES:\n",
        "            nomes_classes = NOMES_CLASSES\n",
        "        dir_yolo, classes_detectadas = converter_voc_para_yolo(\n",
        "            xmls, dir_yolo, por_stem, por_nome, nomes_classes=nomes_classes\n",
        "        )\n",
        "        print('Classes detectadas no VOC:', classes_detectadas)\n",
        "        return dir_yolo, classes_detectadas\n",
        "\n",
        "    pasta_pares = encontrar_pasta_com_pares(dir_base)\n",
        "    if pasta_pares is None:\n",
        "        raise FileNotFoundError('Nenhuma pasta com imagens e labels (.txt/.xml) encontrada.')\n",
        "\n",
        "    imgs = [p for p in pasta_pares.iterdir() if p.is_file() and p.suffix.lower() in EXTENSOES_IMAGEM]\n",
        "    pares = []\n",
        "    for img in imgs:\n",
        "        label = (pasta_pares / f'{img.stem}.txt')\n",
        "        if label.exists():\n",
        "            pares.append((img, label))\n",
        "\n",
        "    if not pares:\n",
        "        raise FileNotFoundError('Nenhum par imagem/label encontrado na pasta detectada.')\n",
        "\n",
        "    dir_yolo = Path('/content/data_yolo')\n",
        "    criar_estrutura_yolo(pares, dir_yolo)\n",
        "    return dir_yolo, None\n",
        "\n",
        "def gerar_data_yaml(dir_base, nomes_classes):\n",
        "    caminho = dir_base / 'data.yaml'\n",
        "    linhas = [\n",
        "        f'path: {dir_base}',\n",
        "        'train: images/train',\n",
        "        'val: images/val',\n",
        "        '',\n",
        "        'names:'\n",
        "    ]\n",
        "    for i, nome in enumerate(nomes_classes):\n",
        "        linhas.append(f'  {i}: {nome}')\n",
        "    caminho.write_text('\\n'.join(linhas), encoding='utf-8')\n",
        "    return caminho\n",
        "\n",
        "if MODO == 'treino':\n",
        "    DIR_DATASET_TREINO, CLASSES_DETECTADAS = preparar_dataset_yolo(DIR_EXTRACAO)\n",
        "    print('Dataset preparado em:', DIR_DATASET_TREINO)\n",
        "\n",
        "    NOMES_CLASSES = []\n",
        "    if 'CLASSES_DETECTADAS' in globals() and CLASSES_DETECTADAS:\n",
        "        NOMES_CLASSES = CLASSES_DETECTADAS\n",
        "    else:\n",
        "        NOMES_CLASSES = [\n",
        "            'user', 'server', 'database', 'api', 'gateway', 'queue',\n",
        "            'storage', 'third_party', 'auth', 'frontend', 'backend', 'network'\n",
        "        ]\n",
        "\n",
        "    CAMINHO_DATA_YAML = gerar_data_yaml(DIR_DATASET_TREINO, NOMES_CLASSES)\n",
        "    print('data.yaml gerado em:', CAMINHO_DATA_YAML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3B) Upload de imagem \u00fanica (somente infer\u00eancia)\n",
        "Envie uma imagem (.png ou .jpg) do diagrama de arquitetura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODO != 'inferencia':\n",
        "    print('Pulando upload de imagem (modo n\u00e3o \u00e9 infer\u00eancia).')\n",
        "else:\n",
        "    ARQUIVOS_IMG = files.upload()\n",
        "    NOME_IMG = next(iter(ARQUIVOS_IMG))\n",
        "    CAMINHO_IMAGEM = str(Path(NOME_IMG).resolve())\n",
        "    print('Imagem carregada:', CAMINHO_IMAGEM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Treinamento do detector (YOLOv8) \u2014 opcional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PESOS_BASE = 'yolov8n.pt'\n",
        "modelo = YOLO(PESOS_BASE)\n",
        "\n",
        "if MODO == 'treino':\n",
        "    if 'CAMINHO_DATA_YAML' not in globals():\n",
        "        raise ValueError('CAMINHO_DATA_YAML n\u00e3o definido. Verifique a prepara\u00e7\u00e3o do dataset.')\n",
        "    # modelo.train(data=CAMINHO_DATA_YAML, epochs=50, imgsz=640)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Infer\u00eancia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODO == 'inferencia':\n",
        "    PESOS_INFERENCIA = 'yolov8n.pt'  # ou '/content/runs/detect/train/weights/best.pt'\n",
        "    modelo = YOLO(PESOS_INFERENCIA)\n",
        "\n",
        "    resultados = modelo.predict(source=CAMINHO_IMAGEM, conf=0.25)\n",
        "    deteccoes = []\n",
        "    for r in resultados:\n",
        "        for c in r.boxes.cls.tolist():\n",
        "            deteccoes.append(modelo.names[int(c)])\n",
        "\n",
        "    MAPA_COMPONENTES = {\n",
        "        'user': 'usu\u00e1rio',\n",
        "        'server': 'servidor',\n",
        "        'database': 'base de dados',\n",
        "        'api': 'api',\n",
        "        'gateway': 'gateway',\n",
        "        'queue': 'fila',\n",
        "        'storage': 'armazenamento',\n",
        "        'third_party': 'terceiros',\n",
        "        'auth': 'autentica\u00e7\u00e3o',\n",
        "        'frontend': 'frontend',\n",
        "        'backend': 'backend',\n",
        "        'network': 'rede'\n",
        "    }\n",
        "\n",
        "    deteccoes_pt = [MAPA_COMPONENTES.get(d, d) for d in deteccoes]\n",
        "    print('Detec\u00e7\u00f5es (bruto):', deteccoes)\n",
        "    print('Detec\u00e7\u00f5es (PT-BR):', deteccoes_pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza\u00e7\u00e3o das detec\u00e7\u00f5es\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'resultados' in globals() and resultados:\n",
        "    img_plot = resultados[0].plot()\n",
        "    img_plot = cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img_plot)\n",
        "    plt.axis('off')\n",
        "else:\n",
        "    print('Sem resultados para visualizar.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Mapeamento STRIDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAPA_COMPONENTES = {\n",
        "    'user': 'usu\u00e1rio',\n",
        "    'server': 'servidor',\n",
        "    'database': 'base de dados',\n",
        "    'api': 'api',\n",
        "    'gateway': 'gateway',\n",
        "    'queue': 'fila',\n",
        "    'storage': 'armazenamento',\n",
        "    'third_party': 'terceiros',\n",
        "    'auth': 'autentica\u00e7\u00e3o',\n",
        "    'frontend': 'frontend',\n",
        "    'backend': 'backend',\n",
        "    'network': 'rede'\n",
        "}\n",
        "\n",
        "MAPA_STRIDE = {\n",
        "    'user': ['Falsifica\u00e7\u00e3o de identidade', 'Eleva\u00e7\u00e3o de privil\u00e9gio'],\n",
        "    'auth': ['Falsifica\u00e7\u00e3o de identidade', 'Eleva\u00e7\u00e3o de privil\u00e9gio', 'Rep\u00fadio'],\n",
        "    'api': ['Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Rep\u00fadio', 'Nega\u00e7\u00e3o de servi\u00e7o'],\n",
        "    'gateway': ['Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Rep\u00fadio', 'Nega\u00e7\u00e3o de servi\u00e7o'],\n",
        "    'database': ['Divulga\u00e7\u00e3o de informa\u00e7\u00e3o', 'Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)'],\n",
        "    'storage': ['Divulga\u00e7\u00e3o de informa\u00e7\u00e3o', 'Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)'],\n",
        "    'queue': ['Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Nega\u00e7\u00e3o de servi\u00e7o'],\n",
        "    'frontend': ['Falsifica\u00e7\u00e3o de identidade', 'Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Divulga\u00e7\u00e3o de informa\u00e7\u00e3o'],\n",
        "    'backend': ['Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Rep\u00fadio', 'Nega\u00e7\u00e3o de servi\u00e7o'],\n",
        "    'network': ['Falsifica\u00e7\u00e3o de identidade', 'Divulga\u00e7\u00e3o de informa\u00e7\u00e3o', 'Nega\u00e7\u00e3o de servi\u00e7o'],\n",
        "    'server': ['Viola\u00e7\u00e3o (manipula\u00e7\u00e3o)', 'Nega\u00e7\u00e3o de servi\u00e7o', 'Eleva\u00e7\u00e3o de privil\u00e9gio'],\n",
        "    'third_party': ['Falsifica\u00e7\u00e3o de identidade', 'Divulga\u00e7\u00e3o de informa\u00e7\u00e3o', 'Rep\u00fadio']\n",
        "}\n",
        "\n",
        "def mapear_stride(componentes):\n",
        "    saida = {}\n",
        "    for c in componentes:\n",
        "        saida[c] = MAPA_STRIDE.get(c, [])\n",
        "    return saida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Gera\u00e7\u00e3o de relat\u00f3rio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "componentes = []\n",
        "if 'deteccoes' in globals() and deteccoes:\n",
        "    componentes = deteccoes\n",
        "else:\n",
        "    componentes = ['api', 'database', 'user']\n",
        "\n",
        "mapeamento = mapear_stride(componentes)\n",
        "\n",
        "linhas = ['# Relat\u00f3rio de Modelagem de Amea\u00e7as', '']\n",
        "for comp in componentes:\n",
        "    comp_label = MAPA_COMPONENTES.get(comp, comp)\n",
        "    ameacas = mapeamento.get(comp, [])\n",
        "    linhas.append(f'## {comp_label}')\n",
        "    if ameacas:\n",
        "        for a in ameacas:\n",
        "            linhas.append(f'- {a}')\n",
        "    else:\n",
        "        linhas.append('- Nenhuma amea\u00e7a mapeada')\n",
        "    linhas.append('')\n",
        "\n",
        "relatorio = '\\n'.join(linhas)\n",
        "print(relatorio)\n",
        "\n",
        "with open('relatorio.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(relatorio)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}